{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_8.ipynb",
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1BUVjyzSx0tqcNLmlIbmVKjAsNFipMGki",
      "authorship_tag": "ABX9TyP10CXK68spnRM71Lmjh+ZY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/falconlee236/DeepLearningFrom_Scratch/blob/main/Book_2/ch08/Chapter_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY715aKQejGI"
      },
      "source": [
        "# Chapter 8 Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVPG-RS6fSbT"
      },
      "source": [
        "**8.1 Structure of Attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiDFO25ZiDL3"
      },
      "source": [
        "8.1.3 Decoder Improvement 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOBSOYJXiICV"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "T, H = 5, 4\n",
        "hs = np.random.randn(T, H)\n",
        "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
        "\n",
        "ar = a.reshape(5, 1).repeat(4, axis=1) # -> (5, 1 * 4)\n",
        "print(ar.shape)\n",
        "\n",
        "t = hs * ar\n",
        "print(t.shape)\n",
        "\n",
        "c = np.sum(t, axis=0)\n",
        "print(c.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ock64IFkizof"
      },
      "source": [
        "N, T, H = 10, 5, 4\n",
        "hs = np.random.randn(N, T, H)\n",
        "a = np.random.randn(N, T)\n",
        "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
        "# or ar = a.reshape(N, T, 1)\n",
        "\n",
        "t = hs * ar\n",
        "print(t.shape)\n",
        "\n",
        "c = np.sum(t, axis=1)\n",
        "print(c.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzQhQvjolFuC"
      },
      "source": [
        "class WeightSum:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, hs, a):\n",
        "        N, T, H = hs.shape\n",
        "\n",
        "        ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
        "        t = hs * ar\n",
        "        c = np.sum(t,axis=1)\n",
        "\n",
        "        self.cache = (hs, ar)\n",
        "        return c\n",
        "    \n",
        "    def backward(self, dc):\n",
        "        hs, ar = self.cache\n",
        "        N, T, H = hs.shape\n",
        "        \n",
        "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
        "        dar = dt * hs\n",
        "        dhs = dt * ar\n",
        "        da = np.sum(dar, axis=2) # backward of repeat\n",
        "\n",
        "        return dhs, da"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upNlKlU1mGhH"
      },
      "source": [
        "8.1.4 Decoder Improvement 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5rD1t8Bwhqb"
      },
      "source": [
        "%cd drive/MyDrive/DeepLearningFrom_Scratch/Book_2/ch08/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLQhoGf0vse2"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.layers import Softmax\n",
        "import numpy as np\n",
        "\n",
        "N, T, H = 10, 5, 4\n",
        "hs = np.random.randn(N, T, H)\n",
        "h = np.random.randn(N, H)\n",
        "hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
        "# or hr = h.reshape(N, 1, H)\n",
        "\n",
        "t = hs * hr\n",
        "print(t.shape)\n",
        "\n",
        "s = np.sum(t, axis=2)\n",
        "print(s.shape)\n",
        "\n",
        "softmax = Softmax()\n",
        "a = softmax.forward(s)\n",
        "print(a.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMStXcMFwfus"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.np import * #import numpy as np\n",
        "from common.layers import Softmax\n",
        "\n",
        "class AttentionWeight:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.softmax = Softmax()\n",
        "        self.cache = None\n",
        "    \n",
        "    def forward(self, hs, h):\n",
        "        N, T, H = hs.shape\n",
        "\n",
        "        hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
        "        t = hs * hr\n",
        "        s = np.sum(t, axis=2)\n",
        "        a = self.softmax.forward(s)\n",
        "\n",
        "        self.cache = (hs, hr)\n",
        "        return a\n",
        "\n",
        "    def backward(self, da):\n",
        "        hs, hr = self.cache\n",
        "        N, T, H = hs.shape\n",
        "\n",
        "        ds = self.softmax.backward(da)\n",
        "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
        "        dhs = dt * hr\n",
        "        dhr = dt * hs\n",
        "        dh = np.sum(dhr, axis=1)\n",
        "\n",
        "        return dhs, dh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlofOWvt020u"
      },
      "source": [
        "8.1.5 Decoder Improvement 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDDp5ukJ1ENl"
      },
      "source": [
        "class Attention:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.attention_weight_layer = AttentionWeight()\n",
        "        self.weight_sum_layer = WeightSum()\n",
        "        self.attention_weight = None\n",
        "\n",
        "    def forward(self, hs, h):\n",
        "        a = self.attention_weight_layer.forward(hs, h)\n",
        "        out = self.weight_sum_layer.forward(hs, a)\n",
        "        self.attention_weight = a\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
        "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
        "        dhs = dhs0 + dhs1\n",
        "        return dhs, dh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K53xLuR92ECg"
      },
      "source": [
        "class TimeAttention:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.layers = None\n",
        "        self.attention_weights = None\n",
        "    \n",
        "    def forward(self, hs_enc, hs_dec):\n",
        "        N, T, H = hs_dec.shape\n",
        "        out = np.empty_like(hs_dec)\n",
        "        self.layers = []\n",
        "        self.attention_weights = []\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = Attention()\n",
        "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])\n",
        "            self.layers.append(layer)\n",
        "            self.attention_weights.append(layer.attention_weight)\n",
        "\n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        N, T, H = dout.shape\n",
        "        dhs_enc = 0\n",
        "        dhs_dec = np.empty_like(dout)\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = self.layers[t]\n",
        "            dhs, dh = layer.backward(dout[:, t, :])\n",
        "            dhs_enc += dhs\n",
        "            dhs_dec[:, t, :] = dh\n",
        "\n",
        "        return dhs_enc, dhs_dec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSspBFeC3tAl"
      },
      "source": [
        "**8.2 seq2seq implementation ready to attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvd_IDbXau3x"
      },
      "source": [
        "*8.2.1 Encoder Implementation*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PejlFNVXayOF"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.time_layers import *\n",
        "from ch07.seq2seq import Encoder, Seq2seq\n",
        "\n",
        "class AttentionEncoder(Encoder):\n",
        "    def forward(self, xs):\n",
        "        xs = self.embed.forward(xs)\n",
        "        hs = self.lstm.forward(xs)\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        dout = self.lstm.backward(dhs)\n",
        "        dout = self.embed.backward(dout)\n",
        "        return dout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx7aBi40chhW"
      },
      "source": [
        "class AttentionDecoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(2 * H, V) / np.sqrt(2 * H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.attention = TimeAttention()\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "        \n",
        "    def forward(self, xs, enc_hs):\n",
        "        h = enc_hs[:, -1] # == enc_hs[:, -1, :]\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        dec_hs = self.lstm.forward(out)\n",
        "        c = self.attention.forward(enc_hs, dec_hs)\n",
        "        out = np.concatenate((c, dec_hs), axis=2)\n",
        "        score = self.affine.forward(out)\n",
        "\n",
        "        return score\n",
        "    \n",
        "    def backward(self, dscore):\n",
        "        dout = self.affine.backward(dscore)\n",
        "        H, T, H2 = dout.shape\n",
        "        H = H2 // 2\n",
        "\n",
        "        dc, ddec_hs0 = dout[:, :, :H], dout[:, :, H:]\n",
        "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
        "        ddec_hs = ddec_hs0 + ddec_hs1\n",
        "        dout = self.lstm.backward(ddec_hs)\n",
        "        dh = self.lstm.dh\n",
        "        denc_hs[:, -1] += dh\n",
        "        self.embed.backward(dout)\n",
        "\n",
        "        return denc_hs\n",
        "\n",
        "    def generate(self, enc_hs, start_id, sample_size):\n",
        "        sampled = []\n",
        "        sample_id = start_id\n",
        "        h = enc_hs[:, -1]\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array([sample_id]).reshape((1, 1))\n",
        "\n",
        "            out = self.embed.forward(x)\n",
        "            dec_hs = self.lstm.forward(out)\n",
        "            c = self.attention.forward(enc_hs, dec_hs)\n",
        "            out = np.concatenate((c, dec_hs), axis=2)\n",
        "            score = self.affine.forward(out)\n",
        "\n",
        "            sample_id = np.argmax(score.flatten())\n",
        "            sampled.append(sample_id)\n",
        "        \n",
        "        return sampled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5lSQLLgjbp7"
      },
      "source": [
        "*8.2.3 seq2seq implemetation*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phgMsgLZiMQv"
      },
      "source": [
        "from ch07.seq2seq import Encoder, Seq2seq\n",
        "\n",
        "class AttentionSeq2seq(Seq2seq):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        args = vocab_size, wordvec_size, hidden_size\n",
        "        self.encoder = AttentionEncoder(*args)\n",
        "        self.decoder = AttentionDecoder(*args)\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "        self.params = self.encoder.params + self.decoder.params\n",
        "        self.grads = self.encoder.grads + self.decoder.grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbRWmv-IjaFY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
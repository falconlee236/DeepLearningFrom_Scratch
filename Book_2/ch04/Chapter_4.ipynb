{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_4.ipynb",
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1_HrpAfDX0rma33Xhuz4POvv4V3v1EGqR",
      "authorship_tag": "ABX9TyMuCkQIaBabZGUNZ7IwNoHv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/falconlee236/DeepLearningFrom_Scratch/blob/main/Book_2/ch04/Chapter_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mw-iS2h2s8Q"
      },
      "source": [
        "# Chapter 4 word2vec speed improvement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvNuUKqn3Ap2"
      },
      "source": [
        "**4.1 word2vec improvement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LMInRjtwkPV"
      },
      "source": [
        "*4.1.2 Embedding layer implementation*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHVQakrXwqzZ"
      },
      "source": [
        "import numpy as np\n",
        "W = np.arange(21).reshape(7, 3)\n",
        "W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn-jFa6nw8iX"
      },
      "source": [
        "W[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hvs1DkySw-X4"
      },
      "source": [
        "W[5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8Ivcdc3w_6H"
      },
      "source": [
        "idx = np.array([1, 0, 3, 0])\n",
        "W[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVWFrTE5xKcX"
      },
      "source": [
        "class Embedding:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.idx = None\n",
        "    \n",
        "    def forward(self, idx):\n",
        "        W, = self.params\n",
        "        self.idx = idx\n",
        "        out = W[idx]\n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        dW, = self.grads\n",
        "        dW[...] = 0\n",
        "\n",
        "        np.add.at(dW, self.idx, dout)\n",
        "\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqzUO2njxdFA"
      },
      "source": [
        "**4.2 word2vec improvement 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6cxLjlM5jWb"
      },
      "source": [
        "*4.2.4 from multi-class classification to binary classification(implementation)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UaG6k1_-nxd"
      },
      "source": [
        "class EmbeddingDot:\n",
        "    def __init__(self, W):\n",
        "        self.embed = Embedding(W)\n",
        "        self.params = self.embed.params\n",
        "        self.grads = self.embed.grad\n",
        "        self.cache = None\n",
        "    \n",
        "    def forward(self, h, idx):\n",
        "        target_W = self.embed.forwad(idx)\n",
        "        out = np.sum(target_W * h, axis=1)\n",
        "\n",
        "        self.cache = (h, target_W)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        h, target_W = self.cache\n",
        "        dout = dout.reshape(dout.shape[0], 1)\n",
        "        \n",
        "        dtarget_W = dout * h\n",
        "        self.embed.backward(dtarget_W)\n",
        "        dh = dout * target_W\n",
        "        return dh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aonl2SdAQaw"
      },
      "source": [
        "*4.2.6 Sampling technique of Negative Sampling *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdpmG9pTllAa"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# random sampling number between 0 to 9 \n",
        "np.random.choice(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz29Go8hlxC3"
      },
      "source": [
        "np.random.choice(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-SYwy3flzsw"
      },
      "source": [
        "# random one number sampling from words list\n",
        "words = ['you', 'say', 'goodbye', 'I', 'hello', '.']\n",
        "np.random.choice(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAe1UCD1mE5H"
      },
      "source": [
        "# random 5 number sampling (multiple selection exist)\n",
        "np.random.choice(words, size=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nmwMTefmZgt"
      },
      "source": [
        "# random 5 number sampling (multiple selection not exist)\n",
        "np.random.choice(words, size=5, replace=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXE37IEomfeI"
      },
      "source": [
        "# sampling about probability distribution\n",
        "p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
        "np.random.choice(words, p=p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1qGKGW8mx5m"
      },
      "source": [
        "p = [0.7, 0.29, 0.01]\n",
        "new_p = np.power(p, 0.75)\n",
        "new_p /= sum(new_p)\n",
        "print(new_p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Id0YUmYn1qs"
      },
      "source": [
        "%cd drive/MyDrive/DeepLearningFrom_Scratch/Book2/ch04"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz6Xnw_eqIjs"
      },
      "source": [
        "from negative_sampling_layer import UnigramSampler\n",
        "\n",
        "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n",
        "power = 0.75\n",
        "sample_size = 2\n",
        "\n",
        "sampler = UnigramSampler(corpus, power, sample_size)\n",
        "target = np.array([1, 3, 0])\n",
        "negative_sample = sampler.get_negative_sample(target)\n",
        "print(negative_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKZ4o4y-qusn"
      },
      "source": [
        "*4.2.7 Negative Sampling implementation*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-SfdlG4yaUE"
      },
      "source": [
        "class NegativeSamplingLoss:\n",
        "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
        "        self.sample_size = sample_size\n",
        "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
        "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
        "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
        "        self.params, self,grads = [], []\n",
        "        for layer in self.embed_dot_layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "    \n",
        "    def forward(self, h, target):\n",
        "        batch_size = target.shape[0]\n",
        "        negative_sample = self.sampler.get_negative_sample(target)\n",
        "\n",
        "        # positive example forwarding\n",
        "        score = self.embed_dot_layers[0].forward(h, target)\n",
        "        correct_label = np.ones(batch_size, dype=np.int32)\n",
        "        loss = self.loss_layers[0].forward(score, correct_label)\n",
        "\n",
        "        # negative example forwarding\n",
        "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
        "        for i in range(self.sample_size):\n",
        "            negative_target = negative_sample[:, i]\n",
        "            score = self.embed_dot_layers[i + 1].forward(h, negative_target)\n",
        "            loss += self.loss_layers[i + 1].forward(score, negative_label)\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def backward(self, dout=1):\n",
        "        dh = 0\n",
        "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
        "            dscore = l0.backward(dout)\n",
        "            dh += l1.backward(dscore)\n",
        "        \n",
        "        return dh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHijzsq12hUM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
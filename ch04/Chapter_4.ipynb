{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_4.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/falconlee236/DeepLearningFrom_Scratch/blob/main/ch04/Chapter_4.ipynb",
      "authorship_tag": "ABX9TyNHAVKjjMEid1zDorPDyplk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/falconlee236/DeepLearningFrom_Scratch/blob/main/ch04/Chapter_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtcpyRwjq7-l"
      },
      "source": [
        "# Chapter 4 Neural network training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyJnf8f-rVJa"
      },
      "source": [
        "**4.2 loss function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blNw53ZmuNVo"
      },
      "source": [
        "*4.2.1 sum of squares for error, SSE*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DeRlukauVul"
      },
      "source": [
        "def sum_squares_error(y, t):\n",
        "  return 0.5 * np.sum((y - t) ** 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UabZVwXmulBt"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# answer is 2\n",
        "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "# example1 : estimate probablity of number 2 to most highest probablity(0.6)\n",
        "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
        "sum_squares_error(np.array(y), np.array(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLTN5A0evhCj"
      },
      "source": [
        "# example2 : estimate probablity of number 7 to most highest probablity(0.6)\n",
        "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
        "sum_squares_error(np.array(y), np.array(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8usI2xlv0XE"
      },
      "source": [
        "the smaller value of loss function is, the better answer has"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq2L5BrLwkZz"
      },
      "source": [
        "*4.2.2 cross entropy error, CEE*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y74U4Jg0wrcK"
      },
      "source": [
        "def cross_entropy_error(y, t):\n",
        "  delta = 1e-7 # log0 = -inf/ protect -inf\n",
        "  return -np.sum(t * np.log(y + delta))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gRzdyqPxktW"
      },
      "source": [
        "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
        "cross_entropy_error(np.array(y), np.array(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPMR6iztxqkL"
      },
      "source": [
        "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
        "cross_entropy_error(np.array(y), np.array(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnvlVWd8x-Kn"
      },
      "source": [
        "*4.2.3 mini-batch training*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79kPIRd-z-g0"
      },
      "source": [
        "%cd drive/MyDrive/DeepLearningFrom_Scratch/ch04/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51CJEnZQ0Pio"
      },
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "print(x_train.shape) # (60000, 784)\n",
        "print(t_train.shape) # (60000, 784)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5XyebTs1pFr"
      },
      "source": [
        "train_size = x_train.shape[0]\n",
        "batch_size = 10\n",
        "batch_mask = np.random.choice(train_size, batch_size)\n",
        "x_batch = x_train[batch_mask]\n",
        "t_batch = t_train[batch_mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKWRr4kv2ZoP"
      },
      "source": [
        "np.random.choice(60000, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aK1vPHv2ckt"
      },
      "source": [
        "# Case: t label is given by one-hot encoding\n",
        "def cross_entropy_error(y, t):\n",
        "  if y.ndim == 1:\n",
        "    t = t.reshape(1, t.size)\n",
        "    y = y.reshape(1, y.size)\n",
        "\n",
        "  batch_size = y.shape[0]\n",
        "  return -np.sum(t * np.log(y + 1e-7)) / batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXwuwt4r568i"
      },
      "source": [
        "# Case: t label is given by number label\n",
        "def cross_entropy_error(y, t):\n",
        "  if y.ndim == 1:\n",
        "    t = t.reshape(1, t.size)\n",
        "    y = y.reshape(1, y.size)\n",
        "\n",
        "  batch_size = y.shape[0]\n",
        "  return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W4K2Zyr6HaN"
      },
      "source": [
        "**4.3 numerical differentiation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GItihRsY7ajd"
      },
      "source": [
        "*4.3.1 differentiation*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwYP6HdP7eaF"
      },
      "source": [
        "# wrong implementation exaple\n",
        "def numerical_diff(f, x):\n",
        "  h = 10e-50\n",
        "  return (f(x + h) - f(x)) / h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy6rXCRZbUtR"
      },
      "source": [
        "# rounding error example\n",
        "np.float32(1e-50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74-vybiMbt-k"
      },
      "source": [
        "# middle differentiation -> Has little error\n",
        "def numerical_diff(f, x):\n",
        "  h = 1e-4 # 0.0001 best delta value\n",
        "  return (f(x + h) - f(x - h)) / (2 * h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toCgJmS7cojB"
      },
      "source": [
        "*4.3.2 exapmle of numerical differentiation*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWu5Pm9Mc1wA"
      },
      "source": [
        "# y = 0.01x**2 + 0.1x\n",
        "def function_1(x):\n",
        "  return 0.01 * x ** 2 + 0.1 * x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RITbq_I9dB9-"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.arange(0.0, 20.0, 0.1) # made array from 0 to 20 for 0.1 space\n",
        "y = function_1(x)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGG7b1AIdd63"
      },
      "source": [
        "numerical_diff(function_1, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOBI41Ppdm6G"
      },
      "source": [
        "numerical_diff(function_1, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz0Tx-Sgdps3"
      },
      "source": [
        "*4.3.3 partial derivative*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjouIrOueIkd"
      },
      "source": [
        "# f(x0, x1) = x0 ** 2 + x1 ** 2\n",
        "def function_2(x):\n",
        "  return np.sum(x ** 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuAv5l7jehXd"
      },
      "source": [
        "# x0 = 3, x1 = 4 -> parital derivative of x0\n",
        "def function_tmp1(x0):\n",
        "  return x0 * x0 + 4.0 ** 2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-glm03vge6jU"
      },
      "source": [
        "numerical_diff(function_tmp1, 3.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kffTEmjjhEiE"
      },
      "source": [
        "# x0 = 3, x1 = 4 -> partial derivative of x1\n",
        "def function_tmp2(x1):\n",
        "  return 3.0 ** 2.0 + x1 * x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXM3fZOVhOXO"
      },
      "source": [
        "numerical_diff(function_tmp2, 4.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKyiU7rjhSVh"
      },
      "source": [
        "**4.4 Gradient**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOa5g80Chg1p"
      },
      "source": [
        "Gradient is vector of differentation array of every variable "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcyY4Ih4h1nA"
      },
      "source": [
        "def numerical_gradient(f, x):\n",
        "  h = 1e-4\n",
        "  grad = np.zeros_like(x) # Generate array like shape x\n",
        "\n",
        "  for idx in range(x.size):\n",
        "    tmp_val = x[idx]\n",
        "    # calculate f(x + h)\n",
        "    x[idx] = tmp_val + h\n",
        "    fxh1 = f(x)\n",
        "\n",
        "    # calculate f(x - h)\n",
        "    x[idx] = tmp_val - h\n",
        "    fxh2 = f(x)\n",
        "\n",
        "    grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
        "    x[idx] = tmp_val # restore value\n",
        "    \n",
        "  return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvAhgiVjjPdf"
      },
      "source": [
        "numerical_gradient(function_2, np.array([3.0, 4.0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTgdEtjQkeVa"
      },
      "source": [
        "numerical_gradient(function_2, np.array([0.0, 2.0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPVjtPpwlI3C"
      },
      "source": [
        "numerical_gradient(function_2, np.array([3.0, 0.0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHGKW50Xl7t_"
      },
      "source": [
        "# f: function, init_x: init value, lr: learning rate, step_num: repetition number\n",
        "def gradient_desent(f, init_x, lr=0.01, step_num=100):\n",
        "  x = init_x # reference value\n",
        "\n",
        "  for i in range(step_num):\n",
        "    grad = numerical_gradient(f, x)\n",
        "    x -= lr * grad\n",
        "  return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxpLPYKcXzlu"
      },
      "source": [
        "init_x = np.array([-3.0, 4.0])\n",
        "gradient_desent(function_2, init_x=init_x, lr=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhTEcIZOYDV0"
      },
      "source": [
        "# example of too much learning rate: lr = 10.0\n",
        "init_x = np.array([-3.0, 4.0])\n",
        "gradient_desent(function_2, init_x=init_x, lr=10.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPA_s-xAYU7R"
      },
      "source": [
        "# example of too less learning rate: lr = 1e-10\n",
        "init_x = np.array([-3.0, 4.0])\n",
        "gradient_desent(function_2, init_x=init_x, lr=1e-10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHgFF59gY0Lf"
      },
      "source": [
        "*4.4.2 gradient of neural network*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE4rJ750blOC"
      },
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from common.functions import softmax, cross_entropy_error\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "class simpleNet:\n",
        "  def __init__(self):\n",
        "    self.W = np.random.randn(2, 3) # initialize normal distribution\n",
        "  \n",
        "  def predict(self, x):\n",
        "    return np.dot(x, self.W)\n",
        "  \n",
        "  def loss(self, x, t):\n",
        "    z = self.predict(x)\n",
        "    y = softmax(z)\n",
        "    loss = cross_entropy_error(y, t)\n",
        "    return loss\n",
        "\n",
        "net = simpleNet()\n",
        "print(net.W)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uexcls2liSpZ"
      },
      "source": [
        "x = np.array([0.6, 0.9])\n",
        "p = net.predict(x)\n",
        "print(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OrT3YfGiwA-"
      },
      "source": [
        "np.argmax(p) # index of Maximum value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "987kNm_Di1-L"
      },
      "source": [
        "t = np.array([0, 0, 1]) # correct label\n",
        "net.loss(x, t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vngP_cXPi5yw"
      },
      "source": [
        "def f(W):\n",
        "  return net.loss(x, t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVOgNSTGjXob"
      },
      "source": [
        "dW = numerical_gradient(f, net.W)\n",
        "print(dW)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaP2ju49jw7c"
      },
      "source": [
        "f = lambda w: net.loss(x, t)\n",
        "dW = numerical_gradient(f, net.W)\n",
        "dW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nf6ZIQ3lhBm"
      },
      "source": [
        "**4.5 Implementing training Algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EztI3fSy-tp"
      },
      "source": [
        "*4.5.1 implementing 2layer neural network class*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUQkE-AJzjei"
      },
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "from common.functions import *\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std =0.01):\n",
        "    #initalize weight\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "  def predict(self, x):\n",
        "    W1, W2 = self.params['W1'], self.params['W2']\n",
        "    b1, b2 = self.params['b1'], self.params['b2']\n",
        "\n",
        "    a1 = np.dot(x, W1) + b1\n",
        "    z1 = sigmoid(a1)\n",
        "    a2 = np.dot(z1, W2) + b2\n",
        "    y = softmax(a2)\n",
        "\n",
        "    return y\n",
        "\n",
        "  # x: input data, t: answer label\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    return cross_entropy_error(y, t)\n",
        "\n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    y = np.argmax(y, axis=1)\n",
        "    t = np.argmax(t, axis=1)\n",
        "\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "    return accuracy\n",
        "\n",
        "  # x: input data, t: answer label\n",
        "  def numerical_gradient(self, x, t):\n",
        "    loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KetksbN2xuy"
      },
      "source": [
        "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
        "print(net.params['W1'].shape)\n",
        "print(net.params['b1'].shape)\n",
        "print(net.params['W2'].shape)\n",
        "print(net.params['b2'].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj_Ga9po3Mi4"
      },
      "source": [
        "x = np.random.rand(100, 784) # dummy input data(around 100page)\n",
        "y = net.predict(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4dfy8N93W3R"
      },
      "source": [
        "x = np.random.rand(100, 784) # dummy input data(around 100page)\n",
        "t = np.random.rand(100, 10) # dummy answer label(around 100page)\n",
        "\n",
        "grads = net.numerical_gradient(x, t) # calculate gradient\n",
        "\n",
        "print(grads['W1'].shape)\n",
        "print(grads['W1'].shape)\n",
        "print(grads['W1'].shape)\n",
        "print(grads['W1'].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5riV548C39D8"
      },
      "source": [
        "*4.5.2 implementing mini-batch training*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GXKM4eS5yPc"
      },
      "source": [
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "train_loss_list = []\n",
        "\n",
        "#hyperparameter\n",
        "iters_num = 10000 # repetition number\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100 # mini-batch size\n",
        "learning_rate = 0.1\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "for i in range(iters_num):\n",
        "  # mini-batch obtain\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  # gradient calculate\n",
        "  grad = network.numerical_gradient(x_batch, t_batch)\n",
        "\n",
        "  # renew parameter\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "  # write training progress\n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_1MpGn38F44"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}